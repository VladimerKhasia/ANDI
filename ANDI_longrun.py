# -*- coding: utf-8 -*-
"""andi_longrun.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-cGuaSsoDBYHVgLS_0BTNooFZKZU2IRP
"""

#@title Long run with biggest affordable model 3 experiments with Andi (Literature Baselines + ANDI Search)
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import math
import matplotlib.pyplot as plt
import numpy as np
import random
import os
import requests
import time
from dataclasses import dataclass

# ==========================================
# 0. CONFIGURATION
# ==========================================
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
DATA_DIR = "./data"
os.makedirs(DATA_DIR, exist_ok=True)

# Steps need to be sufficient to show the "escape" from saddle points
TRAIN_STEPS = 2000
EVAL_INTERVAL = 50
BATCH_SIZE = 128
# We use fewer seeds for the search phase to save time,
# but you can increase this for the final paper run.
SEEDS = [42, 1337]

print(f"Running ANDI Publication Benchmark on: {DEVICE}")
print(f"Training Steps: {TRAIN_STEPS}")

def seed_everything(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

# ==========================================
# 1. OPTIMIZERS (ANDI & MUON)
# ==========================================

class ANDI(optim.Optimizer):
    """
    ANDI (Self-Equilibration).
    Normalizes the gradient by the marginals of the *shifted* topology.
    """
    def __init__(self, params, lr=0.02, momentum=0.9, nesterov=True):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov)
        super().__init__(params, defaults)

    @torch.no_grad()
    def step(self, closure=None):
        loss = None
        if closure is not None:
          with torch.enable_grad(): loss = closure()

        for group in self.param_groups:
            lr, mom, nest = group['lr'], group['momentum'], group['nesterov']
            for p in group['params']:
                if p.grad is None: continue
                g = p.grad

                # Structural Processing for Matrices (Conv2d/Linear)
                if g.ndim > 1:
                    original_shape = g.shape
                    g_mat = g.reshape(g.shape[0], -1)
                    rows, cols = g_mat.shape

                    if rows > 16 and cols > 16:
                        # 1. Prime Mixing
                        prime_shift = 7
                        g_mix = torch.roll(g_mat, shifts=prime_shift, dims=-1)

                        # 2. Self-Equilibration (Norms of the MIXED view)
                        r_norm = g_mix.norm(dim=1, keepdim=True) + 1e-8
                        c_norm = g_mix.norm(dim=0, keepdim=True) + 1e-8
                        g_white = g_mix / (r_norm + c_norm)

                        # Restore
                        g_white = torch.roll(g_white, shifts=-prime_shift, dims=-1)

                        # 3. Soft-Unit Scaling (Hypotenuse)
                        in_norm = g.norm() + 1e-8
                        target = torch.hypot(in_norm, torch.tensor(1.0, device=DEVICE))
                        g_final = g_white * (target / (g_white.norm() + 1e-8))
                        g_final = g_final.view(original_shape)
                    else:
                        target = torch.hypot(g.norm(), torch.tensor(1.0, device=DEVICE))
                        g_final = g * (target / (g.norm() + 1e-8))
                else:
                    target = torch.hypot(g.norm(), torch.tensor(1.0, device=DEVICE))
                    g_final = g * (target / (g.norm() + 1e-8))

                state = self.state[p]
                if 'momentum_buffer' not in state: state['momentum_buffer'] = g_final.clone()
                buf = state['momentum_buffer']
                buf.mul_(mom).add_(g_final)
                update = g_final.add(buf, alpha=mom) if nest else buf
                p.data.add_(update, alpha=-lr)
        return loss

def newton_schulz_5(G, steps=5, eps=1e-7):
    assert G.ndim == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16() if G.is_cuda and G.dtype == torch.float32 else G
    X /= (X.norm() + eps)
    if G.size(0) > G.size(1): X = X.T; transposed = True
    else: transposed = False

    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A
        X = a * X + B @ X

    if transposed: X = X.T
    return X.to(G.dtype)

class Muon(optim.Optimizer):
    def __init__(self, params, lr=0.02, momentum=0.9, nesterov=True, ns_steps=5):
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        super().__init__(params, defaults)
    @torch.no_grad()
    def step(self, closure=None):
        loss = None
        if closure is not None:
          with torch.enable_grad(): loss = closure()
        for group in self.param_groups:
            for p in group['params']:
                if p.grad is None: continue
                g = p.grad
                if g.ndim > 1:
                    g_mat = g.reshape(g.shape[0], -1)
                    if g_mat.size(0) > 32 and g_mat.size(1) > 32:
                        g_orth = newton_schulz_5(g_mat, steps=group['ns_steps'])
                        scale_factor = max(1, g_mat.size(0)/g_mat.size(1))**0.5
                        g_final = g_orth.view(g.shape) * scale_factor
                    else: g_final = g
                else: g_final = g

                state = self.state[p]
                if 'momentum_buffer' not in state: state['momentum_buffer'] = torch.zeros_like(p)
                buf = state['momentum_buffer']
                buf.mul_(group['momentum']).add_(g_final)
                update = g_final.add(buf, alpha=group['momentum']) if group['nesterov'] else buf
                p.data.add_(update, alpha=-group['lr'])
        return loss

# ==========================================
# 2. MODELS
# ==========================================

# A. Deep Autoencoder (Saddle Point Test)
class DeepAutoencoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(784, 256), nn.Tanh(),
            nn.Linear(256, 64), nn.Tanh(),
            nn.Linear(64, 16), nn.Tanh()  # Bottleneck
        )
        self.decoder = nn.Sequential(
            nn.Linear(16, 64), nn.Tanh(),
            nn.Linear(64, 256), nn.Tanh(),
            nn.Linear(256, 784), nn.Sigmoid()
        )

    def forward(self, x):
        x = x.view(x.size(0), -1)
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded

# B. ResNet-9 (Spatial Test)
class ResNet9(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        def conv_block(in_c, out_c, pool=False):
            layers = [nn.Conv2d(in_c, out_c, 3, padding=1, bias=False),
                      nn.BatchNorm2d(out_c), nn.ReLU(inplace=True)]
            if pool: layers.append(nn.MaxPool2d(2))
            return nn.Sequential(*layers)
        self.conv1 = conv_block(3, 64)
        self.conv2 = conv_block(64, 128, pool=True)
        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))
        self.conv3 = conv_block(128, 256, pool=True)
        self.conv4 = conv_block(256, 512, pool=True)
        self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))
        self.classifier = nn.Sequential(nn.MaxPool2d(4), nn.Flatten(), nn.Linear(512, num_classes))

    def forward(self, x):
        out = self.conv1(x)
        out = self.conv2(out)
        out = self.res1(out) + out
        out = self.conv3(out)
        out = self.conv4(out)
        out = self.res2(out) + out
        return self.classifier(out)

# C. NanoGPT (Attention Test)
class NanoGPT(nn.Module):
    def __init__(self, vocab_size, block_size=64, n_embd=128, n_head=4, n_layer=4):
        super().__init__()
        self.token_embedding = nn.Embedding(vocab_size, n_embd)
        self.position_embedding = nn.Embedding(block_size, n_embd)
        self.blocks = nn.Sequential(*[
            nn.Sequential(
                nn.LayerNorm(n_embd),
                nn.MultiheadAttention(n_embd, n_head, batch_first=True),
                nn.LayerNorm(n_embd),
                nn.Sequential(nn.Linear(n_embd, 4*n_embd), nn.GELU(), nn.Linear(4*n_embd, n_embd))
            ) for _ in range(n_layer)
        ])
        self.ln_f = nn.LayerNorm(n_embd)
        self.head = nn.Linear(n_embd, vocab_size, bias=False)
        self.block_size = block_size

    def forward_blocks(self, x):
        B, T, C = x.shape
        attn_mask = torch.triu(torch.ones(T, T, device=x.device) * float('-inf'), diagonal=1)

        for block in self.blocks:
            norm1, attn, norm2, mlp = block
            x_norm = norm1(x)
            attn_out, _ = attn(x_norm, x_norm, x_norm, attn_mask=attn_mask, is_causal=True)
            x = x + attn_out
            x = x + mlp(norm2(x))
        return x

    def forward(self, idx, targets=None):
        B, T = idx.shape
        x = self.token_embedding(idx) + self.position_embedding(torch.arange(T, device=idx.device))
        x = self.forward_blocks(x)
        logits = self.head(self.ln_f(x))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1)) if targets is not None else None
        return logits, loss

# ==========================================
# 3. DATA
# ==========================================

def get_loaders(task):
    print(f"Loading data for task: {task}...")
    if task == "SADDLE":
        tf = transforms.Compose([transforms.ToTensor()])
        ds = torchvision.datasets.FashionMNIST(root=DATA_DIR, train=True, download=True, transform=tf)
        return torch.utils.data.DataLoader(ds, batch_size=128, shuffle=True, pin_memory=True), None

    elif task == "CIFAR":
        tf = transforms.Compose([
            transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(),
            transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
        ])
        ds = torchvision.datasets.CIFAR10(root=DATA_DIR, train=True, download=True, transform=tf)
        return torch.utils.data.DataLoader(ds, batch_size=128, shuffle=True, pin_memory=True), None

    elif task == "GPT":
        path = os.path.join(DATA_DIR, 'tinyshakespeare.txt')
        if not os.path.exists(path):
            print("Downloading TinyShakespeare...")
            r = requests.get("https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt")
            with open(path, 'w') as f: f.write(r.text)
        with open(path, 'r') as f: text = f.read()
        chars = sorted(list(set(text)))
        stoi = {ch:i for i,ch in enumerate(chars)}
        data = torch.tensor([stoi[c] for c in text], dtype=torch.long)

        def get_batch():
            ix = torch.randint(len(data) - 64, (BATCH_SIZE,))
            x = torch.stack([data[i:i+64] for i in ix])
            y = torch.stack([data[i+1:i+64+1] for i in ix])
            return x.to(DEVICE), y.to(DEVICE)
        return get_batch, len(chars)

# ==========================================
# 4. EXPERIMENT ENGINE
# ==========================================

def train_engine(task_name, model_fn, optimizer_cls, opt_kwargs, seeds):
    losses_matrix = []
    loader, vocab_size = get_loaders(task_name)

    for seed in seeds:
        seed_everything(seed)
        if task_name == "GPT": model = model_fn(vocab_size).to(DEVICE)
        else: model = model_fn().to(DEVICE)

        opt = optimizer_cls(model.parameters(), **opt_kwargs)
        seed_losses = []
        if task_name != "GPT": iter_loader = iter(loader)

        start = time.time()
        for i in range(TRAIN_STEPS):
            if task_name == "GPT":
                x, y = loader()
                logits, loss = model(x, y)
            else:
                try: batch = next(iter_loader)
                except StopIteration: iter_loader = iter(loader); batch = next(iter_loader)
                x = batch[0].to(DEVICE)
                if task_name == "SADDLE":
                    recon = model(x)
                    loss = F.mse_loss(recon, x.view(x.size(0), -1))
                else:
                    y = batch[1].to(DEVICE)
                    logits = model(x)
                    loss = F.cross_entropy(logits, y)

            opt.zero_grad()
            loss.backward()
            opt.step()

            if i % EVAL_INTERVAL == 0:
                seed_losses.append(loss.item())

        losses_matrix.append(seed_losses)

    # Return average curve across seeds
    return np.mean(losses_matrix, axis=0)

# ==========================================
# 5. MAIN SUITE (WITH L.R. HEURISTICS & SEARCH)
# ==========================================

def run_suite():
    tasks = ["GPT", "SADDLE", "CIFAR"]
    results = {}

    # CANDIDATE LRs for ANDI (the new method)
    # We will search these and pick the best one automatically.
    andi_lr_search_space = [0.1, 0.05, 0.02, 0.01]

    print("\n>>> STARTING BENCHMARK WITH HEURISTIC BASELINES + ANDI SEARCH")

    for task in tasks:
        print(f"\n==========================================")
        print(f"--- Benchmark Task: {task} ---")
        print(f"==========================================")
        results[task] = {}

        if task == "SADDLE": model_fn = DeepAutoencoder
        elif task == "CIFAR": model_fn = ResNet9
        elif task == "GPT": model_fn = NanoGPT

        # ----------------------------------------------------
        # 1. RUN BASELINES (Known "Best" Parameters)
        # ----------------------------------------------------

        # ADAMW: 3e-4 is standard for Transformers, 1e-3 for standard Vision/MLP
        adam_lr = 3e-4 if task == "GPT" else 1e-3
        print(f"\n > Running AdamW (Heuristic LR: {adam_lr})")
        results[task]["AdamW"] = train_engine(task, model_fn, optim.AdamW,
                                              {"lr": adam_lr, "weight_decay": 1e-4}, SEEDS)

        # MUON: 0.02 is safer for small-batch Transformers, 0.05 is standard for Vision
        muon_lr = 0.02 if task == "GPT" else 0.05
        print(f"\n > Running Muon (Heuristic LR: {muon_lr})")
        results[task]["Muon"] = train_engine(task, model_fn, Muon,
                                             {"lr": muon_lr, "momentum": 0.9}, SEEDS)

        # ----------------------------------------------------
        # 2. RUN ANDI (Search for Best Parameter)
        # ----------------------------------------------------
        print(f"\n > Tuning ANDI (Search Space: {andi_lr_search_space})")

        best_loss = float('inf')
        best_curve = None
        best_lr = 0

        for lr in andi_lr_search_space:
            # We run a quick check using just the first seed to speed up tuning
            print(f"   [Checking LR={lr}]... ", end="")
            curve = train_engine(task, model_fn, ANDI, {"lr": lr}, seeds=[SEEDS[0]])
            final_loss = curve[-1]
            print(f"Final Loss: {final_loss:.4f}")

            if final_loss < best_loss and not np.isnan(final_loss):
                best_loss = final_loss
                best_curve = curve
                best_lr = lr

        print(f"   *** Winner ANDI LR: {best_lr} ***")

        # Optional: Re-run the winner with ALL seeds if you want rigorous error bars
        # For now, we will use the curve we just computed (or re-compute if SEEDS > 1)
        if len(SEEDS) > 1:
            print(f"   Re-running Winner ({best_lr}) with all seeds...")
            best_curve = train_engine(task, model_fn, ANDI, {"lr": best_lr}, SEEDS)

        results[task]["ANDI"] = best_curve

    return results

def plot_final(results):
    # Updated plotting to fix scaling issues
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    titles = {
        "SADDLE": "Deep Autoencoder (Saddle Point Escape)",
        "CIFAR": "ResNet-9 (Spatial Correlation)",
        "GPT": "NanoGPT (Attention Dynamics)"
    }

    for i, (task, data) in enumerate(results.items()):
        ax = axes[i]
        # X-axis based on length of first curve found
        curve_len = len(next(iter(data.values())))
        x = np.arange(curve_len) * EVAL_INTERVAL

        for name, curve in data.items():
            if name == "ANDI": c, s, w = 'green', '-', 2.5
            elif "AdamW" in name: c, s, w = 'blue', ':', 2.0
            elif "Muon" in name: c, s, w = 'orange', '-.', 1.5
            else: c, s, w = 'gray', '--', 1.5

            ax.plot(x, curve, label=name, color=c, linestyle=s, linewidth=w)

        ax.set_title(titles.get(task, task))
        ax.set_xlabel("Steps")
        ax.set_ylabel("Loss")
        ax.grid(True, alpha=0.3)
        ax.legend()

        # --- CORRECTIONS FOR VISUALIZATION ---
        if task == "GPT":
            # Focus on the 0.0 - 5.0 range to see convergence differences
            ax.set_ylim(0, 5.0)
        elif task == "SADDLE":
            # Log scale is essential for Autoencoders to see the "drop"
            ax.set_yscale('log')
        elif task == "CIFAR":
            ax.set_ylim(0, 2.5)

    plt.tight_layout()
    plt.savefig("andi_full_benchmark.png")
    print("\nDone. Plot saved to 'andi_full_benchmark.png'")
    plt.show()

if __name__ == "__main__":
    data = run_suite()
    plot_final(data)